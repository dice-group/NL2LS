{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715af19b-4dbe-45cd-9384-49456b5166ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/upb/users/a/asepff/profiles/unix/cs/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/upb/users/a/asepff/profiles/unix/cs/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bee3c8099434d0db4c9bd6f457502da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98cd819661540dfbd41c0f9707471ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73d7d587fd7404ba0980bbbe4da0424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Translate text into Link Specification:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The link will be generated if the country of the source and the state of the target have maximum overlap Distance or the brotherof of the source and the sibilingsof of target have maximum doublemeta similarity<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "OR(OR(overlapDistance(x.country,y.state)|0.0,OR(AND(brotherof(x,y)|0.0,doublemeta(x,y)|0.0)|0.0,AND(x.country,y.state)|0.0)|0.0)|0.0,doublemeta(x,y)|0.0)|0.0)\n",
      "########################\n",
      "#############\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Translate text into Link Specification:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The link will be generated by computing the average similarity of the minimum value of dice distance between the EducationalOrganization of the source and the insisitutionName of the target or qgrams similarity  between the description of source and the explanation of the target and the maximum value between koeln similarity between the about of the source and the comment of the target or DaitchMokotoff similarity  between the title of source and the name of the target<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "OR(OR(dice(x.educationalOrganization,y.institutionName)|min,OR(OR(qgrams(x.description,y.explanation)|0.5,max(koeln(x.about,y.comment)|1.0)|0.5)|min,daitchMokotoff(x.title,y.name)|1.0)|0.5)|1.0)\n",
      "########################\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoTokenizer, TrainingArguments, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "base_model_name = \"nl2ls_models/checkpoint-1960\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(  ## If it fails at this line, restart the runtime and try again.\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=False,\n",
    "    use_auth_token=False,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "#model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "#model.config.pretraining_tp = 1\n",
    "\n",
    "def generate_answer(example):\n",
    "    \n",
    "    prompt = pipe.tokenizer.apply_chat_template(example[\"messages\"][:2],\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "    terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "    \n",
    "    outputs = pipe(prompt,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                )\n",
    "    generated_text = outputs[0]['generated_text']\n",
    "    return {\"ls\": example['0'], \"generated_text\": generated_text}\n",
    "\n",
    "def create_input_prompt(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\",\"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": example[\"1\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "system_message = \"\"\"\n",
    "    Translate text into Link Specification:\n",
    "    \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
    "    \n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Read CSV files\n",
    "test_df = pd.read_csv(f\"../../../../datasets/New-Datasets/silk-human-annotated/test.txt\", sep=\"\\t\")\n",
    "# Convert DataFrame to Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(create_input_prompt)\n",
    "\n",
    "# Generate answers for the dataset\n",
    "results = test_dataset.map(generate_answer, batched=False)\n",
    "\n",
    "f = open(f\"results.txt\", \"w\")\n",
    "f_ls = open(f\"ls.txt\", \"w\")\n",
    "for result in results:\n",
    "    print(\"#############\")\n",
    "    print(result[\"generated_text\"])\n",
    "    print(\"########################\")\n",
    "    output = result[\"generated_text\"].split(\"\\n\")[-1]\n",
    "    f.write(f\"{output}\\n\")\n",
    "    f_ls.write(f\"{result['ls']}\\n\")\n",
    "f.close()\n",
    "f_ls.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6adda0-9189-40ab-8481-cd26e72e8654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
